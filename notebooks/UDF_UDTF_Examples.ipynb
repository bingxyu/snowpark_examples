{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d0f473-3e84-410e-b9f8-d8cc86dd946c",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDF)\n",
    "\n",
    "There is two diffrent types of UDFs :\n",
    "* UDF (Scalar User Defined Function)\n",
    "    * Is a scalar function that returns one output row for each input row. \n",
    "    * The returned row consists of a single column/value.\n",
    "    * Python UDF batch API enables defining UDFs that receive batches of input rows as Pandas DataFrames and return batches of results as Pandas arrays or Series\n",
    "* UDTF (User Defined Tabular Function)\n",
    "    * A tabular function, also called a table function, returns zero, one, or multiple rows for each input row.\n",
    "\n",
    "\n",
    "A UDF can be created using the **@udf** decorator, the **udf** function or the **udf.register** method ofthe session object. It can be permanent or temporary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a241ea1-cfd9-41b1-bf59-ee17aad33860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# Snowpark imports \n",
    "import snowflake.snowpark as S\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "# Used for reading creds.json\n",
    "import json\n",
    "\n",
    "# Used for UDF examples\n",
    "import cachetools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Used for UDF/UDTF examples\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Used for the UDTF examples\n",
    "from collections import Counter\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "# Print the version of Snowpark we are using\n",
    "print(f\"Using Snowpark: {S.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3717b-7e4a-4838-9da7-0e12bdc6a4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c4ca02-2385-4ad2-945b-93977048509f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Connect to Snowflake\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255d52c-2f3f-4978-a308-f1433e8f37b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(\"Current role: \" + session.get_current_role() + \", Current schema: \" + session.get_fully_qualified_current_schema() + \", Current WH: \" + session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f9855-893b-4a0d-af5b-a1f156e86d42",
   "metadata": {},
   "source": [
    "## UDF\n",
    "\n",
    "Start by creating a UDF that returns a string, by setting is_permanent=False the UDF will only be avalible for our user and also only until the active Snowflake session is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9288e1-9f74-4208-a1b6-75e886761042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "@F.udf(name=\"hello_udf\", is_permanent=False, replace=True, session=session)\n",
    "def hello_udf(name: str) -> str:\n",
    "    return f'Hello {name}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdfe7f-38b7-4476-b9ef-d717843fcc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_df = session.create_dataframe([['Mats'], ['Pia']], schema=[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16477727-2e4e-4ed6-9559-179958ca0288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_df.select(F.call_function(\"hello_udf\", F.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22105e-725a-4087-aab1-db1ce80e8591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "@F.udf(name=\"hello_batch_udf\", is_permanent=False, replace=True, session=session)\n",
    "def hello_batch_udf(pd_df: T.PandasDataFrame[str]) -> T.PandasSeries[str]:\n",
    "    n = len(ds)\n",
    "    return ds.apply(lambda x: f'Hello {x}, we got {n} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b68c42-b2a7-473a-b4bf-0ed890e69598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df = session.table(\"snowflake_sample_data.tpcds_sf100tcl.customer\")\n",
    "print(f\"Nbr of customers: {customers_df.count():,}\")\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19ad88-2fc1-4689-b79c-fc677bcfffd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we test this using **show** we will see that it is not providing 1,000 rows, but 15 since that is the limit we are setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5c5b8-5be5-48b6-bb6c-8c11d6bc7ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df.select(F.col(\"C_FIRST_NAME\")).select(F.call_function(\"hello_batch_udf\", F.col(\"C_FIRST_NAME\"))).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0ac85-92ad-407b-a3d0-382522d95975",
   "metadata": {
    "tags": []
   },
   "source": [
    "By using **cache_result** we can temprary store the result of the query generated by the DataFrame and then seee that each call to the function does provide more rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4a916-3f51-4780-abc0-e21dc14bd395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_udf_df = customers_df.select(F.call_function(\"hello_batch_udf\", F.col(\"C_FIRST_NAME\"))).cache_result()\n",
    "batch_udf_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723be4a-be7e-4d07-b787-52bef1ecf5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading files with UDFs\n",
    "\n",
    "To read a file in a UDF, the file needs to be added using **add_import**.\n",
    "\n",
    "If we do not need to updated the file, we can refeer to a local file and that file will be uploaded to Snowflake when the UDF is created. To use a updated file, we would need to recreate the UDF.\n",
    "\n",
    "By using cachetools wqe can make sure that the file is only loaded once, since cachetools will cache the return object of the function in memory and return it if the paramtere used in teh call is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435e56d-8b06-4fc3-a0d9-e0126569539f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "#data_stage_name = \"~\" # Using the user stage\n",
    "udf_stage_name = \"UDF_DEMO_STAGE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec8876-180b-4395-a4d9-fde41bfae504",
   "metadata": {},
   "source": [
    "Function to read a file from a stage that a UDF has access to, ie the file needs to be added using the imports parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c77f34-36e7-4739-8ae7-5bbae6647011",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cachetools.cached(cache={})\n",
    "def read_file_cached(filename):\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), \"r\") as f:\n",
    "            return f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d240bd8-532d-48d0-b486-2c889ad7387a",
   "metadata": {},
   "source": [
    "Create a UDF where the imports parameter is referring the local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d4978-c961-4e7f-8b84-cbcd3babeb73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "@F.udf(name=\"read_file_static_udf\", is_permanent=False, replace=True, packages=[\"cachetools\"], imports=[f\"{data_path}/text_file.txt\"] ,session=session)\n",
    "def read_file_static() -> str:\n",
    "    return read_file_cached('text_file.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74da6b5-59aa-4907-863b-9ab3204945bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.generator(F.call_function(\"read_file_static_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bb5e8-a232-434c-aa09-2bb31bcfb576",
   "metadata": {},
   "source": [
    "If we want to be able to update the file we need to store it in a Snowflake stage, the stage can be either internal or external.\n",
    "\n",
    "Create a Internal Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a6484-e93c-422b-87d8-434b82e1c8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"create or replace stage {udf_stage_name}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41063811-0dee-4e9d-b11a-71518efd0e6e",
   "metadata": {},
   "source": [
    "Upload a local file to the new stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37180ab-bbfb-4200-96ed-366221f98839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.file.put(f\"{data_path}text_file.txt\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224147e-90a9-490e-aac2-618720d3095c",
   "metadata": {},
   "source": [
    "Check that the file is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec498984-47c4-49a5-96b5-ffca6be4c617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22641a9-fa7f-4d37-ac0f-71e582ff41ce",
   "metadata": {},
   "source": [
    "Create a UDF that hasa access to the file in the stage, using the imports parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2fc74-8079-4aee-87c3-1bb25429750b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "@F.udf(name=\"read_file_stage_udf\", is_permanent=False, replace=True, packages=[\"cachetools\"], imports=[f\"@{udf_stage_name}/text_file.txt\"] ,session=session)\n",
    "def read_file_stage() -> str:\n",
    "    return read_file_cached('text_file.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341848e-96c3-4af5-a2a1-41136ac05f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.generator(F.call_function(\"read_file_stage_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016f547-18f1-4a12-8a86-b17402757ba8",
   "metadata": {},
   "source": [
    "If we change the text_file.txt (in the data folder) and upload it it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140c637-0f3e-435b-988a-ef82a16d8fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.file.put(f\"{data_path}text_file.txt\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)\n",
    "session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa29e1-dcae-4f49-bef5-68e2513cfe5e",
   "metadata": {},
   "source": [
    "Rerun the call to the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689bd3d-9ed6-43d4-9c06-d82c33bcf817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.generator(F.call_function(\"read_file_stage_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a2c44-ea04-47ea-9b6b-e914a3f2f01c",
   "metadata": {},
   "source": [
    "Creating a UDF that uses as saved Python object. In this case a fitted scikit-learn pipline.\n",
    "\n",
    "Create and fit a pipeline, using titanic data (use 00_Load_demo_data.ipynb to load the data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9e25a-c377-4727-b394-4ebdfa5a62fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"EMBARKED\", \"SEX\", \"PCLASS\"]\n",
    "num_cols = [\"AGE\", \"FARE\"]\n",
    "\n",
    "train_df = session.table(\"titanic\").select(*cat_cols, *num_cols, \"SURVIVED\")\n",
    "\n",
    "train_pd = train_df.to_pandas()\n",
    "\n",
    "X = train_pd[[*cat_cols, *num_cols]]\n",
    "y = train_pd[\"SURVIVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1582726-5646-4848-b5dc-bd79cb6c6249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imputer and OneHotEncoder for categorical columns\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "# Imputer and Scaler for numerical columns\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "  [\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ],  verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', RandomForestClassifier())])\n",
    "rc_pipeline = pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3456c2-f4e3-4727-bea0-877a6b0738c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1a9cf-a60a-4784-aee2-ddb8034e3591",
   "metadata": {},
   "source": [
    "Save the fitted pipeline as a file locally using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84937bd7-9e86-4fc6-a241-f40e165e3d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(rc_pipeline, \"rc_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc064fb-500d-4601-8aa7-03c77659e9c4",
   "metadata": {},
   "source": [
    "Upload the file to the Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a87063-29c2-49e6-8d86-4a58b6e54e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.file.put(\"rc_pipeline.joblib\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77df23-e914-4b05-b94a-78d85f9ed1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6962a-4f27-41f8-80c8-17a4ac5204a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create a function to load the file using joblib, use cachetools so the read from stage is only done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c4332-2b75-459c-8562-e4d565f2ffba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cachetools.cached(cache={})\n",
    "def load_joblib_file(filename):\n",
    "    import joblib\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc65f4-7171-4b6c-8d11-fd0908227517",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create the UDF, it is important that the imports parameter is refering the stage and file. Also, only the filename is needed for the load_joblib_file function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad2cc4-95a6-48fb-b872-0b94e2dc3b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = ['pandas', 'scikit-learn==1.1.3', 'cachetools'], replace = True, session = session)\n",
    "def predict_survive(pd_df: T.PandasDataFrame[str, str, str, float, float]) -> T.PandasSeries[int]:\n",
    "    \n",
    "    pd_df.columns = [*cat_cols, *num_cols]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "\n",
    "    return model.predict(pd_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa59a2-8553-414a-ae1f-124ed182638c",
   "metadata": {},
   "source": [
    "Test that the UDF works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa73c0d-5503-416c-b508-226200250feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_cols = [F.col(col) for col in [*cat_cols, *num_cols]]\n",
    "train_df.with_column(\"PREDICTION\", F.call_function(\"predict_survive_udf\", *input_cols)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f1468-447d-421f-bbd8-eb9170980e48",
   "metadata": {},
   "source": [
    "## UDTF\n",
    "\n",
    "User Defined Table Functions (UDTF) is a function that returns zero, one, or multiple rows for each input row.\n",
    "\n",
    "When creating a UDTF a Python class has to be used as the handler\n",
    "\n",
    "A UDTF handler class implements the following, which Snowflake invokes at run time:\n",
    "* An **__init__** method. Optional. Invoked to initialize stateful processing of input partitions.\n",
    "* A **process** method. Required. Invoked for each input row. The method returns a tabular value as tuples.\n",
    "* An **end_partition** method. Optional. Invoked to finalize processing of input partitions.\n",
    "\n",
    "Start with a simple UDTF that splits a string into words and fore each unique word it returns a row with it and the number of ocurrances in the string of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7756d8-7fdc-4470-9ff9-c53530d39354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyWordCount:\n",
    "    # Called once for each partition\n",
    "    def __init__(self):\n",
    "        self._total_per_partition = 0\n",
    "    \n",
    "    # Called for each row in a partition\n",
    "    def process(self, s1: str) -> Iterable[Tuple[str, int]]:\n",
    "        words = s1.split()\n",
    "        self._total_per_partition = len(words)\n",
    "        # Counter will return a dict with the uinique words as keys and the number ocurrances as the values\n",
    "        counter = Counter(words) \n",
    "        yield from counter.items()\n",
    "    \n",
    "    # Called after the last row in a partion has been processed\n",
    "    def end_partition(self):\n",
    "        yield (\"partition_total\", self._total_per_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a09d3b-bab1-44f4-b02f-94321d5e4cda",
   "metadata": {},
   "source": [
    "Create a temporary UDTF. We need to provide the output schema ie the columns of the returning rows. If only names are provided the data types are inheried from the process parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3cc18-1b7e-400f-9579-1b3bd4d8c5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_count_udtf = session.udtf.register(handler=MyWordCount, output_schema=[\"word\", \"count\"], name=\"word_count_udtf\", is_permanent=False, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f853a-f5cb-4449-9192-b856a880e551",
   "metadata": {},
   "source": [
    "Test the UDTF, by using session.table_function we will get a new DataFrame with the data generated by teh UDTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33476c-a11b-410f-b4e8-685d8fa3e852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf = session.table_function(\"word_count_udtf\", F.lit(\"w1 w2 w2 w3 w3 w3\"))\n",
    "df_udtf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11d9aa-d7ac-4bc1-afcc-c4bcab82e104",
   "metadata": {},
   "source": [
    "We can also use it with a DataFrame, using call_table_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36b562-aa76-46b5-b585-8050e576519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_data = session.create_dataframe([[\"w1 w2 w2 w3 w3 w3\"]], schema=[\"text\"])\n",
    "df_udtf_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7572a-028d-479c-84b3-8fe5a1f10025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_data.select(F.call_table_function(\"word_count_udtf\", F.col(\"TEXT\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830123b-d953-4e84-a48c-ec45ffa7d291",
   "metadata": {},
   "source": [
    "If we want to do the split/count by a column, the partition_by parameter can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debeed02-0cde-4462-94f3-ebc4128b8b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_part_data = session.create_dataframe([[\"1\", \"w1 w2 w2 w3 w3 w3\"], [\"2\", \"w4 w4 w4 w4 w1\"]], schema=[\"partition\",\"text\"])\n",
    "df_udtf_part_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ac085-0081-448a-a9ce-6afdc2d76621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_part_data.select(\"partition\", F.call_table_function(\"word_count_udtf\", F.col(\"TEXT\")).over(partition_by=\"partition\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f0f4d-05ad-4eb8-ac7f-3ed120f3d617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1974a8e-cc42-4cb5-a78b-93652c5fcac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03003877-aebd-4d9b-b06a-7daf5b445632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udtf(name=\"collect_list\", is_permanent=False, replace=True, packages=[\"typing\"], output_schema=T.StructType([T.StructField(\"list\", T.ArrayType())]), session=session)\n",
    "class collect_list_handler:\n",
    "    def __init__(self) -> None:\n",
    "        self.list = []\n",
    "    def process(self, element: float) -> Iterable[Tuple[list]]:\n",
    "        self.list.append(element)\n",
    "        yield (self.list,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8fd63-6bad-403c-b88f-178f8b9df4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"collect_list\", F.call_table_function(\"collect_list\", F.col(\"FARE\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf079fb-8081-4139-913a-5c6670f5d57d",
   "metadata": {},
   "source": [
    "We can use a UDTF for doing Scoring, for example if we want to return multiple columns.\n",
    "\n",
    "The example below uses the sklearn pipline we trained earlier to return the probabilities for 0 and 1 and the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cfe4f-e098-47be-9cd4-6b50825a8b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "@F.udtf(name=\"predict_survive_udtf\", is_permanent=False, replace=True, packages=['typing', 'pandas', 'numpy', 'joblib', 'scikit-learn==1.1.3'], imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "        , output_schema=T.StructType([T.StructField(\"prob_0\", T.FloatType()), T.StructField(\"prob_1\", T.FloatType()), T.StructField(\"prediction\", T.StringType())]), session=session)\n",
    "class predict_survive_handler:\n",
    "    # We load the model from stage at the start of each partition\n",
    "    def __init__(self) -> None:\n",
    "        import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "        with open(os.path.join(import_dir, 'rc_pipeline.joblib'), 'rb') as file:\n",
    "            self.model = joblib.load(file)\n",
    "        self.classes = self.model.classes_\n",
    "        \n",
    "    # Score each input row\n",
    "    def process(self, embarked: str, sex: str, pclass: str, age: float, fare: float) -> Iterable[Tuple[float, float, str]]:\n",
    "        # Create a Pandas DataFrame of the input values\n",
    "        pd_df = pd.DataFrame([[embarked, sex, pclass, age, fare]], columns=[\"EMBARKED\",\"SEX\", \"PCLASS\", \"AGE\", \"FARE\"])\n",
    "        \n",
    "        # Get the probabilities for 0/1\n",
    "        prediction_proba = self.model.predict_proba(pd_df)[0]\n",
    "        \n",
    "        # Get the label for the highest probablility\n",
    "        predicted_class_idx = np.argmax(prediction_proba)\n",
    "        predicted_class = self.classes[predicted_class_idx]\n",
    "        \n",
    "        # Create a list with return values\n",
    "        return_list = prediction_proba.tolist()\n",
    "        return_list.append(predicted_class)\n",
    "        \n",
    "        # Return the list as a tuple\n",
    "        yield tuple(return_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be87ec-cf78-4741-82f9-92b63fd71ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.with_column(\"PCLASS\", F.to_varchar(F.col(\"PCLASS\")))\n",
    "train_df.select( *input_cols, F.call_table_function(\"predict_survive_udtf\", *input_cols)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19c8fe-db8a-4194-b116-8327dbbe136a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
